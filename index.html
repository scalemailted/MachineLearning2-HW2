<html>
    <head>
        <link rel="stylesheet" href="styles/bootstrap/bootstrap.min.css"></link>
        <script src="scripts/libs/plotly/plotly-latest.min.js"></script>
        <script src='scripts/libs/math/math.min.js'></script>
        <script>
                MathJax = {
                  tex: {
                    inlineMath: [['$', '$'], ['\\(', '\\)']]
                  }
                };
        </script>
         <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
         <script id="MathJax-script" async
                 src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
         </script>
        <!--<script async src="scripts/libs/mathjax/tex-mml-chtml.js"></script>-->
    </head>
    <body>
        <div class='container-fluid text-center bg-dark text-light'>
            <h1>Ted Holmberg</h1>
            <h4>CSCI 6522, FALL 2019, HW2</h4> 
        </div>
        
        <div class='container-fluid'>
            <h2>Part A.</h2>
            <p>
                <b><i>Task #1:</i></b> 
                Draw the graphs of a sigmoid function $f_{sig}(x) = {1 \over 1+e^{-x}}$ 
                and a hyperbolic tangent function, $f_{tanh}(x) = {e^x-e^{-x} \over {e^x+{e^{-x}}}}$,
                overlapping each other, with x-axis ranging from -10 to +10.
                Describe the differences between them.

            </p>
            <div id="plot" style="width:100%;height:480px;background-color:white;"></div>

            <p>
                <b>Explanation:</b><br>
                Both functions produce 'S'-curves with asymptotic properties on the y-axis. 
                The sigmoid function rapidly approaches a (min) lower limit (min) of 0 
                and (max) upper limit of 1 on the y-axis. The hyperbolic tangent function rapidly
                approaches a (min) lower limit of -1 and a (max) upper limit of 1 on the y-axis.
                See the graphic above for a comparison. 
            </p>
        </div>
        <hr>

        <div class='container-fluid'>
            <h2>Part B.</h2>
            <p>
                <b><i>Task #2:</i></b>
                In chapter 04 (see LECTURE_Chapter_04_Neural_Network_v5) from pages 1-3, and 
                09-13, we have developed the logistic regression (classification) algorithm
                using sigmoid function. Similarly, develop the same in this assignment, however,
                use the hyperbolic tangent function instead of sigmoid function.
                <br>
                <br>
                Formulate the Bernoulli distribution using hyperbolic tangent function:
                the formulation should provide higher score for correct classification, and
                you can relax the perfect formulation of the probability space if needed.
                <br>
                <br>
                You must show by cases, that for correct classification your formulated 
                distribution returns higher value compared to the incorrect classfication.
                Then, for the rest of the part provide your elaborate answer as described
                in the lecture note for sigmoid function.
                <br>
                <br>
                <span style='color:blue;'>
                    Assume, Malignant class label is '+1' and Benign class label is '-1'
                </span>
            </p>

        </div>

        <div class='container-fluid'>
            <h4>Solution:</h4>
            <p>
                In this project, must model a logistic classifier for cancer detection.<br>
                $$ \text{Tumor}  \in \{ \text{Benign}, \text{Malignant} \}$$
                More precisely, we are trying to model:
                $$ P(G|X) $$
                where $X = \{x_1,x_2\}$ and $G \in \{ \text{Benign, Malignant}  \}$,
                $x_1=$ <i>Size of the tumor</i> & $x_2 =$ <i>Age of the tumor</i>. 
                <br>
                We can express the linear equation as:
                $$ X_0\beta_0 + X_1\beta_1 + \cdots + X_p\beta_p = X^T\beta, $$
                Where the $i^{th}$ row in <b>X</b> as : $x_{i}^{T} = (1, x_1,\cdots,x_n)$
                <br><br>
                To convert the output of a linear equation into probability, we can plug it (i.e., $X^T\beta$) into 
                a S-curve function such as the hyperbolic tangent. 
                <br><br>
                Plugging in the probability function:
                $$  P(G|X) = f_{tanh}(X^T\beta) $$
                <br><br>
                We can establish the classification rule based on the above equation as:

                $$  G=1 \text{, if } [f_{tanh}(X^T\beta) \ge 0] \text{, that is } [(X^T\beta) \ge 0] $$
                $$  G=2 \text{, if } [f_{tanh}(X^T\beta) \le 0] \text{, that is } [(X^T\beta) \le 0] $$
                <br><br>
                Logisitic classifier models are usually fit by 
                <span style='color:red;'><i><b>maximum likelihood,</b></i></span>,
                using the conditional likelihood of <i>G</i> given <i>X</i>. Since Pr(<i>G|X</i>) completely specifies
                the conditional distribution. 
                <br><br>
                The log-likelihood for N observations is:
                $$ l(\theta) = \sum\limits_{i=1}^{N} \text{log  }p_{g_{i}}(x_i;\theta) $$
                where $p_k(x_i;\theta)=Pr(G=k|X=x_i;\theta)$
            </p>
        </div>
        <hr>

        <div class='container-fluid'>
            <p><b>Bernoulli distribution:</b></p>
            <p>
                Bernoulli disrtribution is a discrete distribution that results two possible outcomes 
                ( $y \in \{ 0,1 \}$ ) and one of them occurs with probability <i>p</i> whereas the other occurs 
                with probability <i>-p</i>. Such a probability density function can be written as:

                $$ P(y) =  
                    \begin{cases}
                    -p,  & \text{if $y$} < 0  \\
                    +p,   & \text{if $y$} \ge 0
                    \end{cases}
                $$

                In compact form, it can be expressed as:
                $$
                    P(y) = p^{{1 \over 2}(1 + y)} \cdot -p^{{1 \over 2}(1 - y)}
                $$
                We can see that:<br>
                when <i>y</i>=1, we get P(y)=p<br>
                when <i>y</i>=-1, we get P(y)=-p<br>
                <br>
                For binary classification problems, $C_i \in \{ 0,1\}$, we can also, express the distribution as:
                $$ C_i \sim Bernoulli(p) $$

            </p>
        </div>
        <hr>
        <div class='container-fluid'>
            <p>
            Its convenient to code the two-class $g_i$ via a -1/1 response $y_i$, 
            where $y_i=1$ when $g_i=1$ and $y_i=-1$ when $g_i=2$ 
            <br><br>
            Let $p_1(x;\theta)=p(x;\theta)$ and $p_2(x;\theta)=-p(x;\theta)$.
            Also the output can be modeled using the Bernoulli distribution:
            $$ \text{y}_i \sim Bernoulli(\eta_i) $$
            where, $eta_i = f_{tanh}(x^T\beta) = p(x_i;\beta)$
            <br><br>
            Therefore, we can write:
            $$ P(\text{y}_i) = \eta_{i}^{{1 \over 2}(1+\text{y}_i)} \cdot -\eta_{i}^{{1 \over 2}(1-\text{y}_i)} $$
            Hence, the <span class='text-danger'><b><i>likelihood:</i></b></span>
            $$  
                L(\beta) = \prod\limits_{i=1}^{N} P(\text{y}_i) 
                = \prod\limits_{i=1}^{N} 
                    \Big \{
                     \eta_{i}^{{1 \over 2}(1+\text{y}_i)} \cdot -\eta_{i}^{{1 \over 2}(1-\text{y}_i)} 
                    \Big \}
            $$ 
            <span class='text-danger'><b><i>Log likelihood:</i></b></span> may be expressed:
            $$
                l(\beta) = \text{log}L(\beta) = \sum\limits_{i=1}^{N}
                \Big \{
                    {1 \over 2} (1 + \text{y}_i) \text{log}(\eta_i)
                    +
                    {1 \over 2} (1 - \text{y}_i) \text{log}(-\eta_i)
                \Big \}
            $$
            </p>

        </div>
        <hr>
        <div class='container-fluid'>

            <h4>Trigonmentric Properties:</h4>
            <p>
                To properly derive the Logistical Classifer solution, using the hyperbolic tangent function,
                the use of known trigonmentric identities is useful for rapid conversion of our formula.
                See below, for some trigonmetric properties that will be used throughout this solution. 
            </p>
            <br>

            <h4>Hyperbolic Trigonometric Identities:</h4>
            <p>$sinh(x) = {e^x - e^{-x} \over 2}$</p>
            <p>$cosh(x) = {e^x + e^{-x} \over 2}$</p>
            <p>$sech(x) = {1 \over cosh(x)} = {2 \over e^x + e^{-x}}$</p>
            <p>$csch(x) = {1 \over sinh(x)} = {2 \over e^x - e^{-x}}$</p>
            <p>$tanh(x) = {sinh(x) \over cosh(x)} = {e^x - e^{-x} \over e^x + e^{-x}}$</p>
            <p>$coth(x) = {1 \over tanh(x)} = {cosh(x) \over sinh(x)} = {e^x + e^{-x} \over e^x - e^{-x}}$</p>
            <br>

            <h4>Hyperbolic Trigonometric Pythagorean Identities:</h4>
            <p> $cosh^2(x) - sinh^2(x) = 1$</p>
            <p> $tanh^2(x) + sech^2(x) = 1$</p>
            <p> $coth^2(x) - csch^2(x) = 1$</p>
            <br>
            
            <h4>Derivatives for Hyperbolic Trigonometric Identities</h4>
            <p>$ {d \over dx} sinh(x) = cosh(x) $ </p>
            <p>$ {d \over dx} cosh(x) = sinh(x) $ </p>
            <p>$ {d \over dx} tanh(x) = sech^2(x) = 1-tanh^2(x) $</p>
            <p>$ {d \over dx} coth(x) = -csch^2(x) = 1-coth^2(x) $</p>
            <p>$ {d \over dx} sech(x) = -sec(x)tanh(x)$</p>
            <p>$ {d \over dx} csch(x) = -csch(x)coth(x) $</p>
        </div>

        <hr>

        <div class='container-fluid'>
            <h4>Log-likelihood: <i>(Probabilty Density)</i></h4>
            <p>
                <i>Note:</i> Must find a strategy to remove the negative term from rightmost log function<br>
                $
                    l(\beta) = \text{log} \space L(\beta) = \sum\limits_{i=1}^{N}
                    \Bigg \{
                        {1 \over 2} 
                        (1 + \text{y}_i) 
                        \cdot 
                        \text{log}
                        \Big(
                            tanh(X^T\beta)
                        \Big)
                        +
                        {1 \over 2} 
                        (1 - \text{y}_i) 
                        \cdot 
                        \text{log}
                        \Big(
                            -tanh(X^T\beta)
                        \Big)
                    \Bigg \}
                $
            </p>
            <p>
                Use Trigonmentric Identities to take alternate form of tanh function <br> 
                $
                    =
                   \sum\limits_{i=1}^{N}
                    \Bigg \{
                        {1 \over 2} 
                        (1 + \text{y}_i) 
                        \cdot 
                        \text{log}
                        \Big(
                            tanh(X^T\beta)
                        \Big)
                        +
                        {1 \over 2} 
                        (1 - \text{y}_i) 
                        \cdot 
                        \text{log}
                        \Big(
                            - {e^x - e^{-x} \over {e^x + e^{-x}}}
                        \Big)
                    \Bigg \}
                $
            </p>
            <p>
                Distriubte negative sign to numerator<br> 
                $
                    =
                    \sum\limits_{i=1}^{N}
                    \Bigg \{
                        {1 \over 2} 
                        (1 + \text{y}_i) 
                        \cdot 
                        \text{log}
                        \Big(
                            tanh(X^T\beta)
                        \Big)
                        +
                        {1 \over 2} 
                        (1 - \text{y}_i) 
                        \cdot 
                        \text{log}
                        \Big(
                            {e^{-x} - e^{x} \over {e^x + e^{-x}}}
                        \Big)
                    \Bigg \}
                $
            </p>
            <p>
                Since $tanh = {sinh \over cosh}$, then use $cosh$ as denominator term<br> 
                $
                    =
                    \sum\limits_{i=1}^{N}
                    \Bigg \{
                        {1 \over 2} 
                        (1 + \text{y}_i) 
                        \cdot 
                        \text{log}
                        \Big(
                            tanh(X^T\beta)
                        \Big)
                        +
                        {1 \over 2} 
                        (1 - \text{y}_i) 
                        \cdot 
                        \text{log}
                        \Big(
                            {e^{-x} - e^{x} \over cosh(X^T\beta) }
                        \Big)
                    \Bigg \}
                $
            </p>
            <p>
                    Apply sum rule $\sum\{x+y\} = \sum\{x\}+\sum\{y\}$<br> 
                    $
                        =
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} 
                            (1 + \text{y}_i) 
                            \cdot 
                            \text{log}
                            \Big(
                                tanh(X^T\beta)
                            \Big)
                        \Bigg \}
                        +
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} 
                            (1 - \text{y}_i) 
                            \cdot 
                            \text{log}
                            \Big(
                                {e^{-x} - e^{x} \over cosh(X^T\beta) }
                            \Big)
                        \Bigg \}
                    $
                </p> 
        </div>

        <hr>

        <div class='container-fluid'>
            <h4>Gradient: <i>(First-order Derivative)</i></h4>
            <p>
                Next, we need to take the gradient  to get the $\beta$ that maximize the log-likelihood.
                However, since the terms will be longer, we like to compute the gradient by parts of the 
                above equation. Let us compute the derivatives of the two $\sum$ parts.
            </p>
            
            <div>
                <p><b>Left side</b></p>
                <p>
                    $
                        {\partial \over \partial \beta_j}
                        \Bigg(
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} 
                            (1 + \text{y}_i) 
                            \cdot 
                            \text{log}
                            \Big(
                                tanh(X^T\beta)
                            \Big)
                        \Bigg \}
                        \Bigg)
                    $
                </p>
                <p>
                    Apply Rule: ${d \over dx} \sum\{ x\} = \sum\{ {d \over dx}x\}$<br>
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {\partial \over \partial \beta_j}
                            \Bigg(
                            {1 \over 2} 
                            (1 + \text{y}_i) 
                            \cdot 
                            \text{log}
                            \Big(
                                tanh(X^T\beta)
                            \Big)
                        \Bigg)
                        \Bigg \}
                    $
                </p>
                <p>
                    Apply rule: ${d \over dx}(cx) = c{d \over dx}(x)$ to factor out constant term of derivative<br>
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} 
                            (1 + \text{y}_i) 
                            \cdot 
                            {\partial \over \partial \beta_j}
                            \Bigg(
                            \text{log}
                            \Big(
                                tanh(X^T\beta)
                            \Big)
                        \Bigg)
                        \Bigg \}
                    $
                </p>
                <p>
                    Solve log derivative with u substition: ${d \over du}\text{log}(u) = {1 \over u} \cdot ({d \over du} u)$ 
                    ; where $u = tanh(X^T\beta)$<br> 
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} 
                            (1 + \text{y}_i) 
                            \cdot 
                            \Bigg(
                            {1 \over tanh(X^T\beta)}
                            \cdot
                            {\partial \over \partial \beta_j}
                            \Big(
                                tanh(X^T\beta)
                            \Big)
                        \Bigg)
                        \Bigg \}
                    $
                </p>
                <p>
                    Use u substition and trigonmentric derivative rule:
                    ${d \over du}tanh(u)=sech^2(u) \cdot {d \over du}u$
                    <br> 
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} 
                            (1 + \text{y}_i) 
                            \cdot 
                            \Bigg(
                            {1 \over tanh(X^T\beta)}
                            \cdot
                                sech^2(X^T\beta) 
                                \cdot
                                {\partial \over \partial\beta_j}
                                (X^T\beta)
                        \Bigg)
                        \Bigg \}
                    $
                </p>
                <p>
                    Combine trigonmetric terms & solve the derivative w.r.t $\beta_j$ 
                    <br> 
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} 
                            (1 + \text{y}_i) 
                            \cdot 
                            \Bigg(
                            {1 \over tanh(X^T\beta)}
                            \cdot
                                sech^2(X^T\beta) 
                                \cdot x_j
                        \Bigg)
                        \Bigg \}
                    $
                </p>
                <p>
                    Factor $x_j$ term out of parenthetical expression:
                    $(ax) = a(x)$
                    <br> 
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} 
                            (1 + \text{y}_i) 
                            \cdot 
                            x_j
                            \Bigg(
                            {1 \over tanh(X^T\beta)}
                            \cdot
                                sech^2(X^T\beta) 
                        \Bigg)
                        \Bigg \}
                    $
                </p>
                <p>
                    Convert with rules for Trigonmentric Pythagorean identity:
                    $sech^2(x) = [1 - tanh^2(x)]$
                    <br> 
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} 
                            (1 + \text{y}_i) 
                            \cdot 
                            x_j
                            \Bigg(
                            {1 \over tanh(X^T\beta)}
                            \cdot
                                \Big(1 - tanh^2(X^T\beta)\Big)
                        \Bigg)
                        \Bigg \}
                    $
                </p>
                <p>
                    Distribute ${1 \over tanh}$ term
                    <br> 
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} (1 + \text{y}_i) 
                            \cdot x_j
                            \Big(
                                {1 \over tanh(X^T\beta)} 
                                - tanh(X^T\beta)
                            \Big)
                        \Bigg \}
                    $
                </p>
                <p>
                    Combine terms in parenthetical expression
                    <br> 
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} (1 + \text{y}_i) 
                            \cdot x_j
                            \Big(
                                {1 - tanh^2(X^T\beta) \over tanh(X^T\beta)} 
                            \Big)
                        \Bigg \}
                    $
                </p>
                <p>
                    Redefine in terms of $\eta_i$:
                    <br>
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} (1 + \text{y}_i) 
                            \cdot 
                            x_j
                            \big(
                                {1 - \eta^{2}_{i} \over \eta}
                            \big)
                        \Bigg \}
                    $
                </p>
            </div>
            <div>
                <p><b>Right side</b></p>
                <p>
                    $
                        {\partial \over \partial \beta_j}
                        \Bigg(
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} 
                            (1 - \text{y}_i) 
                            \cdot 
                            \text{log}
                            \Big(
                                {e^{-x} - e^{x} \over cosh(X^T\beta) }
                            \Big)
                        \Bigg \}
                        \Bigg)
                    $
                </p>
                <p>
                    Apply Log Rule: $\text{log}({x \over y}) = \text{log}(x) - \text{log}(y)$<br>
                    $
                        {\partial \over \partial \beta_j}
                        \Bigg(
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} 
                            (1 - \text{y}_i) 
                            \cdot 
                            \Big [
                            \text{log}\big( {e^{-x} - e^{x} } \big)
                            -
                            \text{log}\big( cosh(X^T\beta) \big)
                            \Big ]
                        \Bigg \}
                        \Bigg)
                    $
                </p>
                <p>
                    Apply Rule: ${d \over dx} \sum\{ x\} = \sum\{ {d \over dx}x\}$<br>
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {\partial \over \partial \beta_j}
                            \Bigg(
                            {1 \over 2} 
                            (1 - \text{y}_i) 
                            \cdot 
                            \Big [
                            \text{log}\big( {e^{-x} - e^{x} } \big)
                            -
                            \text{log}\big( cosh(X^T\beta) \big)
                            \Big ]
                            \Bigg)
                        \Bigg \}
                    $
                </p>
                <p>
                    Apply rule: ${d \over dx}(cx) = c{d \over dx}(x)$ to factor out constant term of derivative<br>
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} (1 - \text{y}_i) 
                            \cdot 
                            {\partial \over \partial \beta_j}
                            \Bigg(
                            \Big [
                            \text{log}\big( {e^{-x} - e^{x} } \big)
                            -
                            \text{log}\big( cosh(X^T\beta) \big)
                            \Big ]
                            \Bigg)
                        \Bigg \}
                    $
                </p>
                <p>
                    Apply rule: ${d \over dx}(x - y) = {d \over dx}x - {d \over dx}y$<br>
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} (1 - \text{y}_i) 
                            \cdot 
                            
                            \Big [
                            \space
                            \underbrace{ 
                            {\partial \over \partial \beta_j}
                            \text{log}\big( {e^{-x} - e^{x} } \big)
                            \space
                            }_\text{part a}
                            -
                            \underbrace{ 
                            \space
                            {\partial \over \partial \beta_j}
                            \text{log}\big( cosh(X^T\beta) \big)
                            }_\text{part b}
                            \space
                            \Big ]
                        \Bigg \}
                    $
                </p>
                <p>
                    Solve the independent derivatives of part $\text{a}$ and $\text{b}$ <br>
                </p>
                <div>
                    [ <b><i>part a</i></b> ]: 
                    Solve
                    ${\partial \over \partial \beta_j} \text{log}\big( {e^{-x} - e^{x} } \big)$
                    <br>
                    <p>
                        Solve log derivative with u substition: ${d \over du}\text{log}(u) = {1 \over u} \cdot ({d \over du} u)$ 
                        ; where $u = (e^{-x} - e^x)$<br> 
                        $
                            {1 \over (e^{-x} - e^x)} \cdot {\partial \over \partial \beta_j}(e^{-x} - e^x)
                        $
                    </p>
                    <p>
                        Distribute the derivative operator into the parenthetical expression: 
                        ${d \over dx}(a - b) = {d \over dx}a - {d \over dx}b$
                        <br>
                        $
                        {1 \over (e^{-x} - e^x)} 
                        ( 
                        {\partial \over \partial \beta_j}e^{-x} 
                        - 
                        {\partial \over \partial \beta_j}e^x
                        )
                        $
                    </p>
                    <p>
                        Apply rules of exponents: 
                        ${d \over dx}e^{cx} = ce^x$ 
                        <br>
                        $
                        {1 \over (e^{-X^T\beta} - e^{X^T\beta})} 
                        ( 
                        -x_je^{-X^T\beta} 
                        - 
                        x_je^{X^T\beta}
                        )
                        $
                    </p>
                    <p>
                        Factor negative sign from right parenthetical expression: 
                        $(-a - b) = -(a + b)$ 
                        <br>
                        $
                            -{( x_je^{-X^T\beta} + x_je^{X^T\beta} ) 
                        \over 
                            (e^{-X^T\beta} - e^{X^T\beta})
                        } 
                        $
                    </p>
                    <p>
                        Apply negative sign into denominator & factor out $x_j$ in numerator
                        <br>
                        $
                            {(e^{-X^T\beta} + e^{X^T\beta} ) 
                        \over 
                            (-e^{-X^T\beta} + e^{X^T\beta})
                        } \cdot x_j
                        $
                    </p>
                    <p>
                        Use trig identities on numerator & denominator
                        <br>
                        $
                            {(e^{X^T\beta} + e^{-X^T\beta} ) 
                        \over 
                            (e^{X^T\beta} -e^{-X^T\beta} )
                        } \cdot x_j
                        =
                        {cosh(X^T\beta) \over sinh(X^T\beta)} \cdot x_j
                        = coth(X^T\beta) \cdot x_j
                        = {1 \over tanh(X^T\beta)} \cdot x_j
                        $
                    </p>
                </div>
                <div>
                    [ <b><i>part b</i></b> ]:
                    Solve
                    ${\partial \over \partial \beta_j} \text{log}\big( cosh(X^T\beta) \big)$
                    <br>
                    <p>
                        Solve log derivative with u substition: ${d \over du}\text{log}(u) = {1 \over u} \cdot ({d \over du} u)$ 
                        ; where $u = cosh(X^T\beta)$<br> 
                        $
                            =
                            {1 \over cosh(X^T\beta)} \cdot {\partial \over \partial \beta_j}(cosh(X^T\beta))
                        $
                    </p>
                    <p>
                        Use u subsition & chain rule for Trigonmetric derivative: 
                        ${d \over du}cosh(u) = sinh(u) \cdot {d \over du}u  $
                        <br>
                        $
                            =
                            {1 \over cosh(X^T\beta)} 
                            \cdot 
                            \Big( sinh(X^T\beta) 
                            \cdot {\partial \over \partial\beta_j}
                            (X^T\beta)
                            \Big)
                        $
                    </p>
                    <p>
                        Combine trigonmetric terms & solve the derivative w.r.t $\beta_j$ 
                        <br>
                        $
                            =
                            {sinh(X^T\beta) \over cosh(X^T\beta)}  
                            \cdot x_j
                        $
                    </p>
                    <p>
                            Use trigonmentric identities: 
                            ${d \over du}cosh(u) = sinh(u) \cdot {d \over du}u  $
                            <br>
                            $ 
                                = x_j \cdot tanh(X^T\beta)
                            $
                    </p>
                </div>
                <p>
                    [<b><i> part a + b </i></b> ]:
                    Redefine summation with parts $\text{a}$ & $\text{b}$
                    <br>
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} (1 - \text{y}_i) 
                            \cdot 
                            \Big [
                            \underbrace{ 
                            \big (
                                x_j \cdot {1 \over tanh(X^T\beta)}
                            \big ) 
                            }_\text{part a}
                            -
                            \underbrace{ 
                            \big( 
                                x_j \cdot tanh(X^T\beta)
                            \big )
                            }_\text{part b}
                            \space
                            \Big ]
                        \Bigg \}
                    $
                </p>
                <p>
                    Factor out $x_j$ term:
                    $(ax - bx) = x(a - b)$
                    <br>
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} (1 - \text{y}_i) 
                            \cdot 
                            x_j
                            \Big [
                                {1 \over tanh(X^T\beta)}
                                - tanh(X^T\beta)
                            \Big ]
                        \Bigg \}
                    $
                </p>
                <p>
                    Combine terms in parenthetical expression:
                    <br>
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} (1 - \text{y}_i) 
                            \cdot 
                            x_j
                            \Big [
                                {1 - tanh^2(X^T\beta) \over tanh(X^T\beta)}
                            \Big ]
                        \Bigg \}
                    $
                </p>
                <p>
                    Redefine in terms of $\eta_i$:
                    <br>
                    $
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {1 \over 2} (1 - \text{y}_i) 
                            \cdot 
                            x_j
                            \big(
                                {1 - \eta^{2}_{i} \over \eta}
                            \big)
                        \Bigg \}
                    $
                </p>
            </div>
        </div>

        <hr>

        <div class='container-fluid'>
            <h4>Hessian: <i>(Second-order Derivative)</i></h4>
            <p>
                $
                    \text{H} = 
                    {\partial \over \partial \beta_j} 
                    \Big (
                        {\partial \over \partial \beta_j} 
                        \big (
                            l(\beta)
                        \big )
                    \Big )
                    =
                    {\partial \over \partial \beta_j} 
                    \Big (
                        \sum\limits_{i=1}^{N}
                        \Big \{
                            x_j(\eta^{-1} - \eta )
                        \Big \}
                    \Big )
                $
            </p>
            <br>
            <p>
                Define expression in terms of $tanh(X^T\beta)$<br>
                $
                    =
                    {\partial \over \partial \beta_j} 
                    \Bigg (
                        \sum\limits_{i=1}^{N}
                        \Big \{
                            x_{i,j}
                            \Big (
                                coth(X^T\beta) - tanh(X^T\beta)
                            \Big )
                        \Big \}
                    \Bigg )
                $
            </p>
            <p>
                Distribute $x_{i,j}$ term into parenthetical expression<br>
                $
                    =
                    {\partial \over \partial \beta_j} 
                    \Bigg (
                        \sum\limits_{i=1}^{N}
                        \Big \{
                                x_{i,j}coth(X^T\beta) -  x_{i,j}tanh(X^T\beta)
                        \Big \}
                    \Bigg )
                $
            </p>
            <p>
                Apply rule for derivatives on sums: ${d \over dx} \sum \{x\} = \sum\{ {d \over dx}(x) \}$<br>
                $
                    =
                    \sum\limits_{i=1}^{N}
                    \Bigg \{
                        {\partial \over \partial \beta_j} 
                        \Big (
                            x_{i,j}coth(X^T\beta) 
                        \Big )
                        -  
                        {\partial \over \partial \beta_j} 
                        \Big (
                            x_{i,j}tanh(X^T\beta)
                        \Big )
                    \Bigg \}
                $
            </p>
            <p>
                Apply Rules for Trigonmetric Derivatives & apply chain rule for nested term<br>
                $
                    =
                    \sum\limits_{i=1}^{n} \Bigg \{
                    x_j \Big( -csch^2(X^T\beta)) \cdot x_j \Big)  
                    - 
                    x_j \Big( sech^2(X^T\beta)) \cdot x_j \Big) 
                    \Bigg \}
                $
            </p>
            <p>
                Convert with rules for Trigonmentric Pythagorean identity & factor out $x_j$ term<br>
                $
                    =
                    \sum\limits_{i=1}^{n} \Bigg \{
                    x_j \Big( 1 - coth^2(X^T\beta) \Big) x_j 
                    - 
                    x_j \Big( 1 - tanh^2(X^T\beta) \Big) x_j
                    \Bigg \}
                $
            </p>
            <br>
            <p>
                Refactor expression to group parenthetical expressions between $x_j$ terms<br>
                $
                    =
                    \sum\limits_{i=1}^{n} \Bigg \{
                    x_j 
                    \Bigg[
                        \Big( 1 - coth^2(X^T\beta) \Big) 
                        - 
                        \Big( 1 - tanh^2(X^T\beta) \Big) 
                    \Bigg ]
                    x_j
                    \Bigg \}
                $
            </p>
            <br>
            <p>
                Combine nested parenthetical expressions & simplify<br>
                $
                    =
                    \sum\limits_{i=1}^{n} \Bigg \{
                    x_j 
                    \Bigg[
                        \cancel{1} - coth^2(X^T\beta)  
                         \cancel{- 1} + tanh^2(X^T\beta) 
                    \Bigg ]
                    x_j
                    \Bigg \}
                $
            </p>
            <br>
            <p>
                Redefine equation in terms of $tanh(X^T\beta)$ <br>
                $
                    =
                    \sum\limits_{i=1}^{n} \Bigg \{
                    x_j 
                    \Bigg[
                        - {1 \over tanh^2(X^T\beta)  }
                        + tanh^2(X^T\beta) 
                    \Bigg ]
                    x_j
                    \Bigg \}
                $
            </p>
            <br>
            <p>
                Redefine equation in terms of $\eta_i$ <br>
                $
                    =
                    \sum\limits_{i=1}^{n} \Bigg \{
                    x_j
                    \underbrace{ 
                    \Big[
                        \eta^{2}_{i} 
                        - {1 \over \eta^{2}_{i} } 
                    \Big ]
                    }_W
                    x_j
                    \Bigg \}
                $
            </p>

            <p>
                Final solution in terms of W using matrices<br>
                $
                    = X^{T} W X
                $
            </p>
        </div>

        
<!--
        <div class='container-fluid'>
            <h4 class='display'>Part B.</h4>
            <div> 
                    <b>Step 1:</b> <br>
                    Code the two-class $\text{g}_i$ via a -1/1 response $\text{y}_i$,
                    where $\text{y}_i = 1$ when $\text{g}_i = 1$, and $\text{y}_i = -1$ when $\text{g}_i = 2$.        <br>
                                                                                          <br>
                    Let $p_1(x; θ) = p(x; θ)$, and $p_2(x; θ) = 1 − p(x; θ)$.             <br> 
                    We can model the output using the <b><i>Bernoulli distribution</i></b>, i.e., <br>
                    $$ \text{y}_i \space \sim Bernoulli( \eta_i ) ,$$               
                    where,$ \space \eta_i = f_{tanh}(x^T\beta)=p(x_i;\beta)$              <br>                                 <br>
                    Therefore, we can write:                                              <br>
                    $$ 
                        P(\text{y}_i) = 
                        \eta_{i}^{ {1 \over 2}( 1+ \text{y}_i)} 
                        \times
                        - \eta_{i}^{ -{1 \over 2}( \text{y}_i - 1)} 
                    $$
                    Hence,the <b><i>likelihood</i></b> 
                    $
                        L(\beta) =  
                            \prod\limits_{i=1}^{N} P(\text{y}_i)
                            =
                            \prod\limits_{i=1}^{N} 
                                \left(
                                \eta_{i}^{ {1 \over 2}( 1+ \text{y}_i)} 
                                \times
                                - \eta_{i}^{ -{1 \over 2}( \text{y}_i - 1)}
                                \right)
                            
                    $                                                                     <br>
                                                                                          <br>
                    <b><i>Log-likelihood</i></b> can be written:                          <br>
                    $
                        l(\beta) = \text{log} \space L(\beta) = 
                            \sum\limits_{i=1}^{N}
                            \left \{
                                \left [
                                    {1 \over 2}(1+ \text{y}_i) \cdot  \text{log}(\eta_i)
                                \right ]
                                +
                                \left [
                                    -{1 \over 2}(\text{y}_i - 1) \cdot  \text{log}(- \eta_i)
                                \right ]
                            \right \}
                    $                                                                    <br>
                                                                                         <br>
                    $
                     = \sum\limits_{i=1}^{N}
                     \left \{
                         \left [ \space
                             {1 \over 2}(1+ \text{y}_i) \cdot  \text{log}
                             \left (
                                f_{tanh}(x^T,\beta)
                            \right )
                         \right ]
                         +
                         \left [
                             -{1 \over 2}(\text{y}_i - 1) \cdot  \text{log} 
                             \left(
                                 - f_{tanh}(x^T,\beta)
                             \right)
                        \right ]
                     \right \}
                    $                                                                   <br>
                    ,where 
                    $
                        tanh(X^t,\beta) = 
                        { 
                            {e^{X^T \beta} - e^{-X^T \beta}} 
                        \over 
                            {e^{X^T \beta} + e^{-X^T \beta}} 
                        }
                    $                                                                   <br> 
                                                                                        <br>          
            </div>
            <div>
                Next, we need to take the gradient to get the $\beta$ that maximizes the log-likelihood. 
                However, since the terms will be longer, we like to compute the gradient by partsof the 
                above Equation (#todo). Let us compute the derivatives of the first part of (#todo)
                <br>
                $$
                    {\partial \over \partial \beta_j } 
                    \Big( \space
                        l(\beta) 
                    \space \Big)
                $$
                <br>
                
                $$
                    {\partial \over \partial \beta_j } 
                    \Bigg( \space
                        \sum\limits_{i=1}^{N}
                        \left \{
                            \left [ \space
                                {1 \over 2}(1+ \text{y}_i) \cdot  \text{log}
                                \left (
                                f_{tanh}(x^T,\beta)
                            \right )
                            \right ]
                            +
                            \left [
                                -{1 \over 2}(\text{y}_i - 1) \cdot  \text{log} 
                                \left(
                                    - f_{tanh}(x^T,\beta)
                                \right)
                        \right ]
                        \right \} 
                    \space \Bigg)
                $$

                <br> 
                $$
                    {\partial \over \partial \beta_j } 
                    \Bigg( \space
                        \sum\limits_{i=1}^{N}
                        \left \{
                                {1 \over 2}(1+ \text{y}_i) \cdot  \text{log}
                                \left (
                                f_{tanh}(x^T,\beta)
                            \right )
                            \right \}
                        +
                        \sum\limits_{i=1}^{N}
                            \left \{
                                -{1 \over 2}(\text{y}_i - 1) \cdot  \text{log} 
                                \left(
                                    - f_{tanh}(x^T,\beta)
                                \right)
                        \right \} 
                    \space \Bigg)
                $$

                <br> 
                $$
                    {\partial \over \partial \beta_j } 
                    \Bigg( \space
                        \sum\limits_{i=1}^{N}
                        \left \{
                            {1 \over 2}(1+ \text{y}_i) \cdot  \text{log}
                            \Big ( \space
                                f_{tanh}(x^T,\beta)
                            \Big )
                        \right \}
                    \space \Bigg)
                    +
                    {\partial \over \partial \beta_j } 
                    \Bigg( \space
                        \sum\limits_{i=1}^{N}
                            \left \{
                            -{1 \over 2}(\text{y}_i - 1) \cdot  \text{log} 
                            \Big(
                                - f_{tanh}(x^T,\beta)
                            \Big)
                        \right \} 
                    \space \Bigg)
                $$
            </div>

            <div>
                <h4>Left Part:</h4>
                <p>
                    $
                        {\partial \over \partial \beta_j } 
                        \Bigg( \space
                            \sum\limits_{i=1}^{N}
                            \left \{
                                {1 \over 2}(1+ \text{y}_i) \cdot  \text{log}
                                \big ( \space
                                    f_{tanh}(x^T,\beta)
                                \big )
                            \right \}
                        \space \Bigg)
                    $
                </p>
                <p>
                    $
                        =
                        \Big [
                            {1 \over 2}(1+ \text{y}_i)
                        \Big ]
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Bigg( \space
                            \sum\limits_{i=1}^{N}
                            \Big \{
                                \text{log}
                                \big ( \space
                                    f_{tanh}(x^T,\beta)
                                \big )
                            \Big \}
                        \space \Bigg)
                    $
                </p>
                <p>
                    $
                        =
                        \Big [
                            {1 \over 2}(1+ \text{y}_i)
                        \Big ]
                        \cdot
                        \sum\limits_{i=1}^{N}
                        \Bigg \{
                            {\partial \over \partial \beta_j } 
                            \Big( \space
                                \text{log}
                                \big ( \space
                                    f_{tanh}(x^T,\beta)
                                \big )
                            \space \Big)
                        \Bigg \}
                    $
                </p>

                <p>
                    $
                        =
                        \Big [
                            {1 \over 2}(1+ \text{y}_i)
                        \Big ]
                        \cdot
                        \Bigg \{
                            {\partial \over \partial \beta_j } 
                                \Big( \space
                                \text{log}
                                \big ( \space
                                    f_{tanh}(x_1,\beta_1)
                                \big )
                             \space \Big)
                            +
                                \cdots
                            +
                            {\partial \over \partial \beta_j } 
                            \Big( \space
                                \text{log}
                                \big ( \space
                                    f_{tanh}(x_j,\beta_j)
                                \big )
                            \space \Big)
                            +
                                \cdots
                            +
                                {\partial \over \partial \beta_j } 
                                \Big( \space
                                    \text{log}
                                    \big ( \space
                                        f_{tanh}(x_N,\beta_N)
                                    \big )
                                \space \Big)
                        \Bigg \}
                    $
                </p>

                <p>
                    $
                        =
                        \Big [
                            {1 \over 2}(1+ \text{y}_i)
                        \Big ]
                        \cdot
                        \Bigg \{
                            0
                            +
                                \cdots
                            +
                            {\partial \over \partial \beta_j } 
                            \Big( \space
                                \text{log}
                                \big ( \space
                                    f_{tanh}(x_j,\beta_j)
                                \big )
                            \space \Big)
                            +
                                \cdots
                            +
                             0
                        \Bigg \}
                    $
                </p>

                <p>
                    $
                        =
                        \Big [
                            {1 \over 2}(1+ \text{y}_j)
                        \Big ]
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big[ \space
                            \text{log}
                            \big ( \space
                                f_{tanh}(x_j,\beta_j)
                            \big )
                        \space \Big]
                    $
                </p>
                <p>
                    Apply chain rule to solve derivative on nested functions<br>
                    $
                        =
                        \Big [
                            {1 \over 2}(1+ \text{y}_j)
                        \Big ]
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big[ \space
                            \text{log}
                            \big ( \space
                                f_{tanh}(x_j,\beta_j)
                            \big )
                        \space \Big]
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big[ \space
                            f_{tanh}(x_j,\beta_j)
                        \space \Big]
                    $
                    </p>

                <p>
                    Use <i>u</i> substution to solve derivative of log value.<br>
                    $
                       {d \over du}
                       \Big (
                        \text{log}(u)
                       \Big )
                       = {1 \over u}
                    $
                     ; where  
                    $
                        u = f_{tanh}(x_j,\beta_j)
                    $
                    <br>
                    Then resubstitue $f$ back in for $u$,
                    <br>
                    $
                        {\partial \over \partial \beta_j} 
                        \Big [
                            \text{log}
                            \big( 
                                tanh(x_j,\beta_j) 
                            \big )
                        \Big ] 
                        = 
                        {1 \over tanh(x_j,\beta_j)}
                        =
                        coth(x_j,\beta_j)
                    $
                    <br>
                    Thus,
                    <br>
                    $
                        =
                        \Big [
                            {1 \over 2}(1+ \text{y}_j)
                        \Big ]
                        \cdot
                        coth(x_j,\beta_j)
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big( \space
                            tanh(x_j,\beta_j)
                        \space \Big)
                    $
                </p>
                <p>
                    Short answer:<br> 
                    Use trigonmetric rules and chain rule to derive solution instantly:
                    <br>
                    $
                        {\partial \over \partial \beta_j} \Big( tanh(x_j\beta_j) \Big)
                        = {\partial \over \partial \beta_j} \Big( tanh(x_j\beta_j) \Big) 
                        \cdot {\partial \over \partial \beta_j} (x_j\beta_j) 
                        = sech^2(x_j\beta_j) \cdot x_j
                    $

                </p>
                <p>
                    Long answer:<br>
                    Apply the Quotient Rule
                    $ 
                        \big (
                            {f \over g}
                        \big )'
                        =
                        { f' \cdot  g - g' \cdot f \over g^2}
                    $
                    , where
                    $ f = ( {e^{x_j \beta_j} - e^{-x_j \beta_j}} )$
                    $, g =( {e^{x_j \beta_j} + e^{-x_j \beta_j}} )$ 
                    <br>
                    <br>
                    $
                        {\partial \over \partial \beta_j } 
                        \Big( \space
                            tanh(x_j,\beta_j)
                        \space \Big)
                        =
                        {\partial \over \partial \beta_j } 
                        \Big( \space
                        { 
                            {e^{x_j \beta_j} - e^{-x_j \beta_j}} 
                        \over 
                            {e^{x_j \beta_j} + e^{-x_j \beta_j}} 
                        }
                        \space \Big)
                    $
                    <br>
                    <br>
                    $
                    =
                    {
                        {
                            {\partial \over \partial \beta_j}
                            ( \space
                                e^{x_j \beta_j} - e^{-x_j \beta_j}
                            )
                            \cdot
                            ( \space
                            e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )
                            \space
                            -
                            \space
                            ( \space
                            e^{x_j \beta_j} - e^{-x_j \beta_j}
                            )
                            \cdot
                            {\partial \over \partial \beta_j}
                            ( \space
                            e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )

                            
                        }
                        \over
                        {
                            (
                                e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )^2
                        }
                    }
                    $
                </p>
                <p>
                    Distribute derviative operation to each term in parenthesis
                    <br><br>
                    $
                        =
                        {
                            {
                                \Big [
                                {\partial \over \partial \beta_j} ( e^{x_j \beta_j} )
                                \space
                                -
                                \space
                                {\partial \over \partial \beta_j} ( e^{-x_j \beta_j} )
                                \Big ]
                                \cdot
                                ( \space
                                e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )
                                \space
                                -
                                \space
                                ( \space
                                e^{x_j \beta_j} - e^{-x_j \beta_j}
                                )
                                \cdot
                                \Big [
                                {\partial \over \partial \beta_j} ( e^{x_j \beta_j} )
                                \space
                                +
                                \space
                                {\partial \over \partial \beta_j} ( e^{-x_j \beta_j} )
                                \Big ]

                                
                            }
                            \over
                            {
                                (
                                    e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )^2
                            }
                        }
                    $
                </p>
                <p>
                    Use derivatives rules for exponentials to solve numerator
                    where ${d \over dx}(e^{cx}) = c \cdot e^{cx}$
                    <br><br>
                    $
                        =
                        {
                            {
                                \Big [
                                x_j \cdot e^{x_j \beta_j} 
                                \space
                                -
                                \space
                                -x_j \cdot e^{-x_j \beta_j} 
                                \Big ]
                                \cdot
                                ( \space
                                e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )
                                \space
                                -
                                \space
                                ( \space
                                e^{x_j \beta_j} - e^{-x_j \beta_j}
                                )
                                \cdot
                                \Big [
                                 x_j \cdot e^{x_j \beta_j}
                                \space
                                +
                                \space
                                -x_j \cdot e^{-x_j \beta_j} 
                                \Big ]

                                
                            }
                            \over
                            {
                                (
                                    e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )^2
                            }
                        }
                    $
                </p>

                <p>
                    Multiply the binomial expressions together
                    <br><br>
                    $
                        =
                        {
                            {
                                \Big [
                                x_j \cdot e^{x_j \beta_j} 
                                \space
                                +
                                \space
                                x_j \cdot e^{-x_j \beta_j} 
                                \Big ]
                                \cdot
                                ( \space
                                e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )
                                \space
                                -
                                \space
                                ( \space
                                e^{x_j \beta_j} - e^{-x_j \beta_j}
                                )
                                \cdot
                                \Big [
                                    x_j \cdot e^{x_j \beta_j}
                                \space
                                +
                                \space
                                -x_j \cdot e^{-x_j \beta_j} 
                                \Big ]
                            }
                            \over
                            {
                                (
                                    e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )^2
                            }
                        }
                    $
                </p>

                <p>
                    $
                        =
                        {
                            {
                                (
                                x_je^{x_j \beta_j} 
                                \space
                                +
                                \space
                                x_je^{-x_j \beta_j} 
                                )
                                \cdot
                                ( \space
                                e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )
                                \space
                                -
                                \space
                                ( \space
                                e^{x_j \beta_j} - e^{-x_j \beta_j}
                                )
                                \cdot
                                (
                                    x_je^{x_j \beta_j}
                                \space
                                -
                                \space
                                x_je^{-x_j \beta_j} 
                                )
                            }
                            \over
                            {
                                (
                                    e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )^2
                            }
                        }
                    $
                </p>

                <p>
                    $
                        =
                        {
                            {
                                \Big [
                                    x_je^{x_j\beta_j}e^{x_j\beta_j}
                                    + x_je^{x_j\beta_j}e^{-x_j\beta_j}
                                    + x_je^{-x_j\beta_j}e^{x_j\beta_j}
                                    + x_je^{-x_j\beta_j}e^{-x_j\beta_j}
                                \Big ]
                                -
                                \Big [
                                    x_je^{x_j\beta_j}e^{x_j\beta_j}
                                    - x_je^{-x_j\beta_j}e^{x_j\beta_j}
                                    - x_je^{x_j\beta_j}e^{-x_j\beta_j}
                                    + x_je^{-x_j\beta_j}e^{-x_j\beta_j}
                                \Big ]
                            }
                            \over
                            {
                                (
                                    e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )^2
                            }
                        }
                    $
                </p>

                <p>
                    Use exponent rules to merge simplify exponent expressions with same base
                    <br><br>
                    $
                        =
                        {
                            {
                                \Big [
                                    x_je^{2x_j\beta_j}
                                    + x_je^0
                                    + x_je^0
                                    + x_je^{-2x_j\beta_j}
                                \Big ]
                                -
                                \Big [
                                    x_je^{2x_j\beta_j}
                                    - x_je^0
                                    - x_je^0
                                    + x_je^{-2x_j\beta_j}
                                \Big ]
                            }
                            \over
                            {
                                (
                                    e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )^2
                            }
                        }
                    $
                </p>

                <p>
                    Apply rule such that $x^0 = 1$
                    <br><br>
                    $
                        =
                        {
                            {
                                \Big [
                                    x_je^{2x_j\beta_j}
                                    + x_j
                                    + x_j
                                    + x_je^{-2x_j\beta_j}
                                \Big ]
                                -
                                \Big [
                                    x_je^{2x_j\beta_j}
                                    - x_j
                                    - x_j
                                    + x_je^{-2x_j\beta_j}
                                \Big ]
                            }
                            \over
                            {
                                (
                                    e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )^2
                            }
                        }
                    $
                </p>

                <p>
                    Distribute negative sign to righthand expression in numerator
                    <br><br>
                    $
                        =
                        {
                            {
                                \Big [
                                    x_je^{2x_j\beta_j}
                                    + x_j
                                    + x_j
                                    + x_je^{-2x_j\beta_j}
                                \Big ]
                                +
                                \Big [
                                    - x_je^{2x_j\beta_j}
                                    + x_j
                                    + x_j
                                    - x_je^{-2x_j\beta_j}
                                \Big ]
                            }
                            \over
                            {
                                (
                                    e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )^2
                            }
                        }
                    $
                </p>

                <p>
                    Group like terms
                    <br><br>
                    $
                        =
                        {
                            {
                                (
                                    x_je^{2x_j\beta_j}
                                    - x_je^{2x_j\beta_j}
                                )
                                + 
                                (
                                    x_j
                                    + x_j
                                    + x_j
                                    + x_j
                                )
                                + 
                                (
                                    x_je^{-2x_j\beta_j}
                                    - x_je^{-2x_j\beta_j}
                                )
                            }
                            \over
                            {
                                (
                                    e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )^2
                            }
                        }
                    $
                </p>

                <p>
                    Cancel terms and Combine terms
                    <br><br>
                    $
                        =
                        {
                            {
                                \enclose{horizontalstrike}{
                                (
                                    x_je^{2x_j\beta_j}
                                    - x_je^{2x_j\beta_j}
                                )}
                                + \space
                                    4x_j
                                \space +
                                \enclose{horizontalstrike}{ 
                                (
                                    x_je^{-2x_j\beta_j}
                                    - x_je^{-2x_j\beta_j}
                                )}
                            }
                            \over
                            {
                                (
                                    e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )^2
                            }
                        }
                    $
                </p>

                <p>
                    Redefine result as $secth^2(x_j\beta_j)$. Note that: ${d \over dx}\Big(tanh(x)\Big) = {sech(x)^2}$<br>
                    Since $sech(x) = {2 \over e^x + e^{-x}}$ then $sech^2(x) = {4 \over (e^x + e^{-x})^2}$ 
                    <br><br>
                    $
                        =
                        {
                            { 4x_j}
                            \over
                            { ( e^{x_j \beta_j} + e^{-x_j \beta_j} )^2 }
                        }
                    $
                    $
                        =
                        { 4 \over ( e^{x_j \beta_j} + e^{-x_j \beta_j} )^2 }x_j
                    $
                    $
                        = {1 \over cos^2(x_j\beta_j) }x_j
                    $
                    $
                        = sech^2(x_j\beta_j) \cdot x_j
                    $
                    $
                        = x_jsech^2(x_j\beta_j)
                    $
                </p>

                <p>
                    <b>Left Part Solution:</b> <br>
                    $
                        {1 \over 2}(1 + y_j) \cdot coth(x_j\beta_j) \cdot x_jsech^2(x_j\beta_j)
                    $
                </p>
                <p>
                    $
                        = {x_j \over 2}(1 + y_j) 
                        \cdot {cosh(x_j\beta_j) \over sinh(x_j\beta_j) }
                        \cdot {1 \over cosh^2(x_j\beta_j) }
                    $
                </p>
                <p>
                    $
                        = {x_j \over 2}(1 + y_j) 
                        \cdot {\enclose{horizontalstrike}{ cosh(x_j\beta_j)} \over sinh(x_j\beta_j) }
                        \cdot {1 \over cosh(x_j\beta_j) \cdot \enclose{horizontalstrike}{ cosh(x_j\beta_j)} }
                    $
                </p>
                <p>
                        $
                            = {x_j \over 2}(1 + y_j) 
                            \cdot {1 \over sinh(x_j\beta_j)} 
                            \cdot {1 \over cosh(x_j\beta_j)} 
                        $
                    </p>
                <p>
                    $
                        = {x_j \over 2}(1 + y_j) 
                        \cdot csch(x_j\beta_j) 
                        \cdot sech(x_j\beta_j) 
                    $
                </p>
            </div>

            <hr>
            <div>
                    <h4>Right Part:</h4>
                    <p>
                        $
                        {\partial \over \partial \beta_j } 
                        \Bigg( \space
                            \sum\limits_{i=1}^{N}
                                \Big \{
                                -{1 \over 2}(\text{y}_i - 1) \cdot  \text{log} 
                                \Big(
                                    - tanh(x^T\beta)
                                \Big)
                            \Big \} 
                        \space \Bigg)
                        $
                    </p>
                    <p>
                        $
                            =
                                \sum\limits_{i=1}^{N}
                                \Bigg \{
                                {\partial \over \partial \beta_j } 
                                \Big(
                                    -{1 \over 2}(\text{y}_i - 1) \cdot  \text{log} 
                                    \big(
                                        - tanh(x^T\beta)
                                    \big)
                                \Big)
                                \Bigg \} 
                        $
                    </p>
                    <p>
                        $
                            =
                                \sum\limits_{i=1}^{N}
                                \Bigg \{
                                -{1 \over 2}(\text{y}_i - 1) 
                                \cdot
                                {\partial \over \partial \beta_j } 
                                \Big(
                                    \text{log} 
                                    \big(
                                        - tanh(x_i\beta_i)
                                    \big)
                                \Big)
                                \Bigg \} 
                        $
                    </p>
                    <p>
                        $
                            =
                            \Bigg \{
                            -{1 \over 2}(\text{y}_1 - 1) 
                            \cdot
                            {\partial \over \partial \beta_j } 
                            \Big(
                                \text{log} 
                                \big(
                                    - tanh(x_1\beta_1)
                                \big)
                            \Big)

                            +
                            \cdots
                            +

                            -{1 \over 2}(\text{y}_j - 1) 
                            \cdot
                            {\partial \over \partial \beta_j } 
                            \Big(
                                \text{log} 
                                \big(
                                    - tanh(x_j\beta_j)
                                \big)
                            \Big)

                            +
                            \cdots
                            +

                            -{1 \over 2}(\text{y}_N - 1) 
                            \cdot
                            {\partial \over \partial \beta_j } 
                            \Big(
                                \text{log} 
                                \big(
                                    - tanh(x_N\beta_N)
                                \big)
                            \Big)
                            \Bigg \} 

                        $
                    </p>

                    <p>
                        $
                            =
                            \Bigg \{
                            -{1 \over 2}(\text{y}_1 - 1) 
                            \cdot 0 

                            +
                            \cdots
                            +

                            -{1 \over 2}(\text{y}_j - 1) 
                            \cdot
                            {\partial \over \partial \beta_j } 
                            \Big(
                                \text{log} 
                                \big(
                                    - tanh(x_j\beta_j)
                                \big)
                            \Big)

                            +
                            \cdots
                            +
                            
                            -{1 \over 2}(\text{y}_N - 1) 
                            \cdot 0
                            \Bigg \} 

                        $
                </p>

                <p>
                    $
                        =
                        \Bigg \{
                        0 + \cdots +

                        -{1 \over 2}(\text{y}_j - 1) 
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big(
                            \text{log} 
                            \big(
                                - tanh(x_j\beta_j)
                            \big)
                        \Big)

                        + \cdots + 0
                        \Bigg \} 

                    $
                </p>

                <p>
                    $
                        =
                        -{1 \over 2}(\text{y}_j - 1) 
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big(
                            \text{log} 
                            \big(
                                - tanh(x_j\beta_j)
                            \big)
                        \Big)

                    $
                </p>
            </div>

            <div>
                <p>
                    Apply chain rule to solve derivative on nested functions<br>
                    $
                        =
                        -{1 \over 2}(\text{y}_j - 1)
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big( \space
                            \text{log}
                            \big ( \space
                                - tanh(x_j\beta_j)
                            \big )
                        \space \Big)
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big( \space
                            - tanh(x_j\beta_j)
                        \space \Big)
                    $
                </p>

                <p>
                    Use <i>u</i> substution to solve derivative of log value.<br>
                    $
                        {d \over du}
                        \Big (
                        \text{log}(u)
                        \Big )
                        = {1 \over u}
                    $
                        ; where  
                    $
                        u = - tanh(x_j\beta_j)
                    $
                    <br>
                    Then resubstitue $f$ back in for $u$,
                    <br>
                    $
                        {\partial \over \partial \beta_j} 
                        \Big [
                            \text{log}
                            \big( 
                                -tanh(x_j\beta_j) 
                            \big )
                        \Big ] 
                        = 
                        {1 \over -tanh(x_j\beta_j)}
                        =
                        -coth(x_j\beta_j)
                    $
                    <br>
                    Thus,
                    <br>
                    $
                        =
                        - {1 \over 2}(\text{y}_j - 1)
                        \cdot
                        - coth(x_j\beta_j)
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big( \space
                            - tanh(x_j\beta_j)
                        \space \Big)
                    $
                    <br>
                    $
                        =
                        {1 \over 2}(\text{y}_j - 1)
                        \cdot
                        coth(x_j\beta_j)
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big( \space
                            - tanh(x_j\beta_j)
                        \space \Big)
                    $
                </p>
                <p>
                    Short answer:<br> 
                    Use trigonmetric rules and chain rule to derive solution instantly:
                    <br>
                    $
                        {\partial \over \partial \beta_j} \Big( - tanh(x_j\beta_j) \Big)
                        = {\partial \over \partial \beta_j} \Big( - tanh(x_j\beta_j) \Big) 
                        \cdot {\partial \over \partial \beta_j} (x_j\beta_j) 
                        = - sech^2(x_j\beta_j) \cdot x_j
                    $

                </p>
            </div>
            <div>
                $
                    =
                    {1 \over 2}(\text{y}_j - 1)
                    \cdot
                    coth(x_j\beta_j)
                    \cdot
                    - sech^2(x_j\beta_j) \cdot x_j
                $
            </div>
            <div>
                <br>
                $
                    =
                    {x_j \over 2}(1 - \text{y}_j)
                    \cdot
                    coth(x_j\beta_j)
                    \cdot
                    sech^2(x_j\beta_j)
                $
            </div>
            <div>
                <p>
                    $
                        = {x_j \over 2}(1 - \text{y}_j) 
                        \cdot {cosh(x_j\beta_j) \over sinh(x_j\beta_j) }
                        \cdot {1 \over cosh^2(x_j\beta_j) }
                    $
                </p>
                <p>
                    $
                        = {x_j \over 2}(1 - \text{y}_j) 
                        \cdot {\enclose{horizontalstrike}{ cosh(x_j\beta_j)} \over sinh(x_j\beta_j) }
                        \cdot {1 \over cosh(x_j\beta_j) \cdot \enclose{horizontalstrike}{ cosh(x_j\beta_j)} }
                    $
                </p>
                <p>
                        $
                            = {x_j \over 2}(1 - \text{y}_j) 
                            \cdot {1 \over sinh(x_j\beta_j)} 
                            \cdot {1 \over cosh(x_j\beta_j)} 
                        $
                    </p>
                <p>
                    $
                        = {x_j \over 2}(1 - \text{y}_j) 
                        \cdot csch(x_j\beta_j) 
                        \cdot sech(x_j\beta_j) 
                    $
                </p>
            </div>

            <div>
                <h4>Solution</h4>
                <p>
                    $
                    = 
                    \big [
                    {x_j \over 2}(1 + \text{y}_j) 
                    \cdot csch(x_j\beta_j) 
                    \cdot sech(x_j\beta_j) 
                    \big ]
                    +
                    \big [
                    {x_j \over 2}(1 - \text{y}_j) 
                    \cdot csch(x_j\beta_j) 
                    \cdot sech(x_j\beta_j) 
                    \big ]
                    $
                </p>
-->
<!--
                <br>
                <p>
                    $
                    = 
                    {1 \over 2} \big [
                    x_j(1 + \text{y}_j) 
                    \cdot csch(x_j\beta_j) 
                    \cdot sech(x_j\beta_j) 
                    \big ]
                    +
                    {1 \over 2} \big [
                    x_j(1 - \text{y}_j) 
                    \cdot csch(x_j\beta_j) 
                    \cdot sech(x_j\beta_j) 
                    \big ]
                    $
                </p>
-->
<!--
                <br>
                <p>
                    $
                    = 
                    x_j(1 + \text{y}_j) 
                    \cdot csch(x_j\beta_j) 
                    \cdot sech(x_j\beta_j) 
                    $
                </p>
-->
<!--
                <br>
                <p>
                    $
                    = 
                    \big [
                    {x_j \over 2} 
                    \cdot csch(x_j\beta_j) 
                    \cdot sech(x_j\beta_j)
                    \big ]
                    \cdot
                    \big [
                    (1 + \text{y}_j) 
                    \cdot
                    (1 - \text{y}_j) 
                    \big ]
                    $
                </p>

                <br>
                <p>
                    $
                    = 
                    {x_j \over 2 sinh(x_j\beta_j) cosh(x_j \beta_j)} 
                    \cdot
                    \big [
                    (1 + \text{y}_j) 
                    \cdot
                    (1 - \text{y}_j) 
                    \big ]
                    $
                </p>

                <br>
                <p>
                    $
                    = 
                    {x_j \over 2 sinh(x_j\beta_j) cosh(x_j \beta_j)} 
                    \cdot
                    (1 - \text{y}_{j}^{2})
                    $
                </p>
            </div>
-->

            <!-- #ATTEMPT 2-->
<!--
            <div>
                <h4>Attempt 2: MUST RESOLVE NEGATIVE LOF FXN; Impacts Rightside</h4>
                <p>
                    Since you cannot have a negative value within a log term, 
                    must distribute the negative value into the $-\eta_i$ function
                    <br>
                    Given $\eta_i = tanh(X^T\beta) = { e^{X^{T}\beta} - e^{-X^{T}\beta} \over e^{X^{T}\beta} + e^{-X^{T}\beta}}$
                    <br>
                    Thus, 
                    $
                    -\eta_i  = -tanh(X^T\beta)  
                    $
                    $
                    = - \big (  { e^{X^{T}\beta} - e^{-X^{T}\beta} \over e^{X^{T}\beta} + e^{-X^{T}\beta}} \big ) 
                    $
                    $
                    = { -e^{X^{T}\beta} + e^{-X^{T}\beta} \over e^{X^{T}\beta} + e^{-X^{T}\beta}} 
                    $
                </p>
                <br><br>
                <h4>Right Part:</h4>
-->
<!--
                <p>
                    $
                        = -{1 \over 2} (y_j - 1) \cdot {\partial \over  \partial\beta_j}
                        \Big (
                            \text{log}
                            \big (
                                { -e^{X^{T}\beta} + e^{-X^{T}\beta} \over e^{X^{T}\beta} + e^{-X^{T}\beta}}
                            \big )
                        \Big )
                    $
                </p>
-->
<!--
                <p>
                    $
                        = -{1 \over 2} (y_j - 1) \cdot {\partial \over  \partial\beta_j}
                        \Big (
                            \text{log}
                            \big (
                                { -e^{x_j\beta_j} + e^{-x_j\beta_j} \over e^{x_j\beta_j} + e^{-x_j\beta_j}}
                            \big )
                        \Big )
                    $
                    <br><br>
                    $
                        = -{1 \over 2} (y_j - 1) \cdot {\partial \over  \partial\beta_j}
                        \Big (
                            \text{log}
                            \big (
                                { -e^{x_j\beta_j} + e^{-x_j\beta_j} \over cosh(x_j\beta_j) }
                            \big )
                        \Big )
                    $
                </p>
                <br><br>
                <p>
                    Apply chain rule to solve derivative on nested functions<br>
                    $
                        =
                        -{1 \over 2}(\text{y}_j - 1)
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big( \space
                            \text{log}
                            \big ( \space
                                { -e^{x_j\beta_j} + e^{-x_j\beta_j} \over cosh(x_j\beta_j) }
                            \big )
                        \space \Big)
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big( \space
                            { -e^{x_j\beta_j} + e^{-x_j\beta_j} \over cosh(x_j\beta_j) }
                        \space \Big)
                    $
                </p>

                <p>
                    Use <i>u</i> substution to solve derivative of log value.<br>
                    $
                        {d \over du}
                        \Big (
                        \text{log}(u)
                        \Big )
                        = {1 \over u}
                    $
                        ; where  
                    $
                        u = { -e^{x_j\beta_j} + e^{-x_j\beta_j} \over cosh(x_j\beta_j) }
                    $
                    <br>
                    Then resubstitue $f$ back in for $u$,
                    <br>
                    $
                        {\partial \over \partial \beta_j} 
                        \Big [
                            \text{log}
                            \big( 
                                { -e^{x_j\beta_j} + e^{-x_j\beta_j} \over cosh(x_j\beta_j) }
                            \big )
                        \Big ] 
                        = 
                        {1 \over \Big( { -e^{x_j\beta_j} + e^{-x_j\beta_j} \over cosh(x_j\beta_j)} \Big) }
                        =
                        { cosh(x_j\beta_j) \over -e^{x_j\beta_j} + e^{-x_j\beta_j}  }
                    $
                    <br>
                    Thus,
                    <br>
                    $
                        =
                        - {1 \over 2}(\text{y}_j - 1)
                        \cdot
                            { cosh(x_j\beta_j) \over -e^{x_j\beta_j} + e^{-x_j\beta_j}  }
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big( \space
                            { -e^{x_j\beta_j} + e^{-x_j\beta_j} \over cosh(x_j\beta_j) }
                        \space \Big)
                    $
                </p>

                <p>
                    Apply negative sign in expression to redefine sinh function
                    <br>
                    $
                        =
                        {1 \over 2}(\text{y}_j - 1)
                        \cdot
                            { cosh(x_j\beta_j) \over -(-e^{x_j\beta_j} + e^{-x_j\beta_j})  }
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big( \space
                            { -e^{x_j\beta_j} + e^{-x_j\beta_j} \over cosh(x_j\beta_j) }
                        \space \Big)
                    $
                </p>
                <br>
                <p>
                    $
                        =
                        {1 \over 2}(\text{y}_j - 1)
                        \cdot
                            { cosh(x_j\beta_j) \over e^{x_j\beta_j} - e^{-x_j\beta_j}  }
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big( \space
                            { -e^{x_j\beta_j} + e^{-x_j\beta_j} \over cosh(x_j\beta_j) }
                        \space \Big)
                    $
                </p>
                <br>
                <p>
                    $
                        =
                        {1 \over 2}(\text{y}_j - 1)
                        \cdot
                            { cosh(x_j\beta_j) \over sinh(x_j\beta_j)  }
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big( \space
                            { -e^{x_j\beta_j} + e^{-x_j\beta_j} \over cosh(x_j\beta_j) }
                        \space \Big)
                    $
                </p>
                <br>
                <p>
                    $
                        =
                        {1 \over 2}(\text{y}_j - 1)
                        \cdot
                            coth(x_j\beta_j)
                        \cdot
                        {\partial \over \partial \beta_j } 
                        \Big( \space
                            { -e^{x_j\beta_j} + e^{-x_j\beta_j} \over cosh(x_j\beta_j) }
                        \space \Big)
                    $
                </p>
                <p>
                    Long answer:<br>
                    Apply the Quotient Rule
                    $ 
                        \big (
                            {f \over g}
                        \big )'
                        =
                        { f' \cdot  g - g' \cdot f \over g^2}
                    $
                    , where
                    $ f = ( {-e^{x_j \beta_j} + e^{-x_j \beta_j}} )$
                    $, g =cosh(x_j\beta_j)$ 
                </p>
                <br>
                <p>
                    $
                        =
                        {\partial \over \partial \beta_j } 
                        \Big( \space
                        { 
                            {-e^{x_j \beta_j} + e^{-x_j \beta_j}} 
                        \over 
                            cosh(x_j\beta_j) 
                        }
                        \space \Big)
                    $
                    <br>
                    <br>
                </p>
                <p>
        
                        $
                        =
                        {
                            {
                                {\partial \over \partial \beta_j}
                                \Big ( \space
                                    -e^{x_j \beta_j} + e^{-x_j \beta_j}
                                \Big )
                                \cdot
                                cosh(x_j\beta_j)
                                \space
                                -
                                \space
                                ( \space
                                -e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )
                                \cdot
                                {\partial \over \partial \beta_j}
                                \Big ( \space
                                cosh(x_j\beta_j)
                                \Big )
                
                                
                            }
                            \over
                            cosh^2(x_j\beta_j)
                        }
                        $
                    </p>

                <p>
                    Distribute derviative operation to each term in parenthesis 
                    <br><br>
                    $
                        =
                        {
                            {
                                \Big [
                                {\partial \over \partial \beta_j} ( -e^{x_j \beta_j} )
                                \space
                                +
                                \space
                                {\partial \over \partial \beta_j} ( e^{-x_j \beta_j} )
                                \Big ]
                                \cdot
                                cosh(x_j\beta_j)
                                \space
                                -
                                \space
                                ( \space
                                -e^{x_j \beta_j} + e^{-x_j \beta_j}
                                )
                                \cdot
                                \Big [ \space
                                {\partial \over \partial \beta_j}
                                (
                                    cosh(x_j\beta_j)
                                )
                                \Big ]
            
                                
                            }
                            \over
                            cosh^2(x_j\beta_j)
                        }
                    $
                </p>

            </div>
-->



<!-- 


    <p>
        Use derivatives rules for exponentials to solve numerator
        where ${d \over dx}(e^{cx}) = c \cdot e^{cx}$
        <br><br>
        $
            =
            {
                {
                    \Big [
                    -x_j \cdot e^{x_j \beta_j} 
                    \space
                    +
                    \space
                    -x_j \cdot e^{-x_j \beta_j} 
                    \Big ]
                    \cdot
                    cosh(x_j\beta_j)
                    \space
                    -
                    \space
                    ( \space
                    -e^{x_j \beta_j} + e^{-x_j \beta_j}
                    )
                    \cdot
                    \Big [ \space
                    {\partial \over \partial \beta_j}
                    (
                        cosh(x_j\beta_j)
                    )
                    \Big ]

                    
                }
                \over
                {
                    cosh^2(x_j\beta_j)
                }
            }
        $
    </p>

    <p>
        Multiply the binomial expressions together
        <br><br>
        $
            =
            {
                {
                    \Big [
                    x_j \cdot e^{x_j \beta_j} 
                    \space
                    +
                    \space
                    x_j \cdot e^{-x_j \beta_j} 
                    \Big ]
                    \cdot
                    ( \space
                    e^{x_j \beta_j} + e^{-x_j \beta_j}
                    )
                    \space
                    -
                    \space
                    ( \space
                    e^{x_j \beta_j} - e^{-x_j \beta_j}
                    )
                    \cdot
                    \Big [
                        x_j \cdot e^{x_j \beta_j}
                    \space
                    +
                    \space
                    -x_j \cdot e^{-x_j \beta_j} 
                    \Big ]
                }
                \over
                {
                    (
                        e^{x_j \beta_j} + e^{-x_j \beta_j}
                    )^2
                }
            }
        $
    </p>

    <p>
        $
            =
            {
                {
                    (
                    x_je^{x_j \beta_j} 
                    \space
                    +
                    \space
                    x_je^{-x_j \beta_j} 
                    )
                    \cdot
                    ( \space
                    e^{x_j \beta_j} + e^{-x_j \beta_j}
                    )
                    \space
                    -
                    \space
                    ( \space
                    e^{x_j \beta_j} - e^{-x_j \beta_j}
                    )
                    \cdot
                    (
                        x_je^{x_j \beta_j}
                    \space
                    -
                    \space
                    x_je^{-x_j \beta_j} 
                    )
                }
                \over
                {
                    (
                        e^{x_j \beta_j} + e^{-x_j \beta_j}
                    )^2
                }
            }
        $
    </p>

    <p>
        $
            =
            {
                {
                    \Big [
                        x_je^{x_j\beta_j}e^{x_j\beta_j}
                        + x_je^{x_j\beta_j}e^{-x_j\beta_j}
                        + x_je^{-x_j\beta_j}e^{x_j\beta_j}
                        + x_je^{-x_j\beta_j}e^{-x_j\beta_j}
                    \Big ]
                    -
                    \Big [
                        x_je^{x_j\beta_j}e^{x_j\beta_j}
                        - x_je^{-x_j\beta_j}e^{x_j\beta_j}
                        - x_je^{x_j\beta_j}e^{-x_j\beta_j}
                        + x_je^{-x_j\beta_j}e^{-x_j\beta_j}
                    \Big ]
                }
                \over
                {
                    (
                        e^{x_j \beta_j} + e^{-x_j \beta_j}
                    )^2
                }
            }
        $
    </p>

    <p>
        Use exponent rules to merge simplify exponent expressions with same base
        <br><br>
        $
            =
            {
                {
                    \Big [
                        x_je^{2x_j\beta_j}
                        + x_je^0
                        + x_je^0
                        + x_je^{-2x_j\beta_j}
                    \Big ]
                    -
                    \Big [
                        x_je^{2x_j\beta_j}
                        - x_je^0
                        - x_je^0
                        + x_je^{-2x_j\beta_j}
                    \Big ]
                }
                \over
                {
                    (
                        e^{x_j \beta_j} + e^{-x_j \beta_j}
                    )^2
                }
            }
        $
    </p>

    <p>
        Apply rule such that $x^0 = 1$
        <br><br>
        $
            =
            {
                {
                    \Big [
                        x_je^{2x_j\beta_j}
                        + x_j
                        + x_j
                        + x_je^{-2x_j\beta_j}
                    \Big ]
                    -
                    \Big [
                        x_je^{2x_j\beta_j}
                        - x_j
                        - x_j
                        + x_je^{-2x_j\beta_j}
                    \Big ]
                }
                \over
                {
                    (
                        e^{x_j \beta_j} + e^{-x_j \beta_j}
                    )^2
                }
            }
        $
    </p>

    <p>
        Distribute negative sign to righthand expression in numerator
        <br><br>
        $
            =
            {
                {
                    \Big [
                        x_je^{2x_j\beta_j}
                        + x_j
                        + x_j
                        + x_je^{-2x_j\beta_j}
                    \Big ]
                    +
                    \Big [
                        - x_je^{2x_j\beta_j}
                        + x_j
                        + x_j
                        - x_je^{-2x_j\beta_j}
                    \Big ]
                }
                \over
                {
                    (
                        e^{x_j \beta_j} + e^{-x_j \beta_j}
                    )^2
                }
            }
        $
    </p>

    <p>
        Group like terms
        <br><br>
        $
            =
            {
                {
                    (
                        x_je^{2x_j\beta_j}
                        - x_je^{2x_j\beta_j}
                    )
                    + 
                    (
                        x_j
                        + x_j
                        + x_j
                        + x_j
                    )
                    + 
                    (
                        x_je^{-2x_j\beta_j}
                        - x_je^{-2x_j\beta_j}
                    )
                }
                \over
                {
                    (
                        e^{x_j \beta_j} + e^{-x_j \beta_j}
                    )^2
                }
            }
        $
    </p>

    <p>
        Cancel terms and Combine terms
        <br><br>
        $
            =
            {
                {
                    \enclose{horizontalstrike}{
                    (
                        x_je^{2x_j\beta_j}
                        - x_je^{2x_j\beta_j}
                    )}
                    + \space
                        4x_j
                    \space +
                    \enclose{horizontalstrike}{ 
                    (
                        x_je^{-2x_j\beta_j}
                        - x_je^{-2x_j\beta_j}
                    )}
                }
                \over
                {
                    (
                        e^{x_j \beta_j} + e^{-x_j \beta_j}
                    )^2
                }
            }
        $
    </p>

    <p>
        Redefine result as $secth^2(x_j\beta_j)$. Note that: ${d \over dx}\Big(tanh(x)\Big) = {sech(x)^2}$<br>
        Since $sech(x) = {2 \over e^x + e^{-x}}$ then $sech^2(x) = {4 \over (e^x + e^{-x})^2}$ 
        <br><br>
        $
            =
            {
                { 4x_j}
                \over
                { ( e^{x_j \beta_j} + e^{-x_j \beta_j} )^2 }
            }
        $
        $
            =
            { 4 \over ( e^{x_j \beta_j} + e^{-x_j \beta_j} )^2 }x_j
        $
        $
            = {1 \over cos^2(x_j\beta_j) }x_j
        $
        $
            = sech^2(x_j\beta_j) \cdot x_j
        $
        $
            = x_jsech^2(x_j\beta_j)
        $
    </p>

-->



    <!-- #ATTEMPT 3-->
<!--
        <div>
            <h4>Attempt 3: COSH & SINH require scalar of 1/2 to be converted, do this last!</h4>
        </div>



            <p>
                Long answer:<br>
                Apply the Quotient Rule
                $ 
                    \big (
                        {f \over g}
                    \big )'
                    =
                    { f' \cdot  g - g' \cdot f \over g^2}
                $
                , where
                $ f = ( {-e^{x_j \beta_j} + e^{-x_j \beta_j}} )$
                $, g =( {e^{x_j \beta_j} + e^{-x_j \beta_j}} )$ 
                <br>
                <br>
                $
                    {\partial \over \partial \beta_j } 
                    \Big( \space
                        - tanh(x_j,\beta_j)
                    \space \Big)
                    =
                    {\partial \over \partial \beta_j } 
                    \Big( \space
                    - \big (
                    { 
                        {e^{x_j \beta_j} - e^{-x_j \beta_j}} 
                    \over 
                        {e^{x_j \beta_j} + e^{-x_j \beta_j}} 
                    }
                    \big )
                    \space \Big)
                    =
                    {\partial \over \partial \beta_j } 
                    \Big( \space
                    { 
                        {-e^{x_j \beta_j} + e^{-x_j \beta_j}} 
                    \over 
                        {e^{x_j \beta_j} + e^{-x_j \beta_j}} 
                    }
                    \space \Big)
                $
                <br>
                <br>
                $
                =
                {
                    {
                        {\partial \over \partial \beta_j}
                        ( \space
                            -e^{x_j \beta_j} + e^{-x_j \beta_j}
                        )
                        \cdot
                        ( \space
                        e^{x_j \beta_j} + e^{-x_j \beta_j}
                        )
                        \space
                        -
                        \space
                        ( \space
                        -e^{x_j \beta_j} + e^{-x_j \beta_j}
                        )
                        \cdot
                        {\partial \over \partial \beta_j}
                        ( \space
                        e^{x_j \beta_j} + e^{-x_j \beta_j}
                        )

                        
                    }
                    \over
                    {
                        (
                            e^{x_j \beta_j} + e^{-x_j \beta_j}
                        )^2
                    }
                }
                $
            </p>
            <p>
                Distribute derviative operation to each term in parenthesis
                <br><br>
                $
                    =
                    {
                        {
                            \Big [
                            {\partial \over \partial \beta_j} ( -e^{x_j \beta_j} )
                            \space
                            +
                            \space
                            {\partial \over \partial \beta_j} ( e^{-x_j \beta_j} )
                            \Big ]
                            \cdot
                            ( \space
                            e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )
                            \space
                            -
                            \space
                            ( \space
                            -e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )
                            \cdot
                            \Big [
                            {\partial \over \partial \beta_j} ( e^{x_j \beta_j} )
                            \space
                            +
                            \space
                            {\partial \over \partial \beta_j} ( e^{-x_j \beta_j} )
                            \Big ]

                            
                        }
                        \over
                        {
                            (
                                e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )^2
                        }
                    }
                $
            </p>
            <p>
                Use derivatives rules for exponentials to solve numerator
                where ${d \over dx}(e^{cx}) = c \cdot e^{cx}$
                <br><br>
                $
                    =
                    {
                        {
                            \Big [
                            -x_j \cdot e^{x_j \beta_j} 
                            \space
                            +
                            \space
                            -x_j \cdot e^{-x_j \beta_j} 
                            \Big ]
                            \cdot
                            ( \space
                            e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )
                            \space
                            -
                            \space
                            ( \space
                            -e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )
                            \cdot
                            \Big [
                             x_j \cdot e^{x_j \beta_j}
                            \space
                            +
                            \space
                            -x_j \cdot e^{-x_j \beta_j} 
                            \Big ]

                            
                        }
                        \over
                        {
                            (
                                e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )^2
                        }
                    }
                $
            </p>

            <p>
                Multiply the binomial expressions together
                <br><br>
                $
                    =
                    {
                        {
                            (
                            -x_j \cdot e^{x_j \beta_j} 
                            \space
                            -
                            \space
                            x_j \cdot e^{-x_j \beta_j} 
                            )
                            \cdot
                            ( \space
                            e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )
                            \space
                            -
                            \space
                            ( \space
                            -e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )
                            \cdot
                            (
                                x_j \cdot e^{x_j \beta_j}
                            \space
                            -
                            \space
                            x_j \cdot e^{-x_j \beta_j} 
                            )
                        }
                        \over
                        {
                            (
                                e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )^2
                        }
                    }
                $
            </p>

            <p>
                $
                    =
                    {
                        {
                            \Big [
                                - x_je^{x_j\beta_j}e^{x_j\beta_j}
                                - x_je^{x_j\beta_j}e^{-x_j\beta_j}
                                - x_je^{-x_j\beta_j}e^{x_j\beta_j}
                                - x_je^{-x_j\beta_j}e^{-x_j\beta_j}
                            \Big ]
                            -
                            \Big [
                                - x_je^{x_j\beta_j}e^{x_j\beta_j}
                                + x_je^{-x_j\beta_j}e^{x_j\beta_j}
                                + x_je^{x_j\beta_j}e^{-x_j\beta_j}
                                - x_je^{-x_j\beta_j}e^{-x_j\beta_j}
                            \Big ]
                        }
                        \over
                        {
                            (
                                e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )^2
                        }
                    }
                $
            </p>

            <p>
                Use exponent rules to merge simplify exponent expressions with same base
                <br><br>
                $
                    =
                    {
                        {
                            \Big [
                                -x_je^{2x_j\beta_j}
                                - x_je^0
                                - x_je^0
                                - x_je^{-2x_j\beta_j}
                            \Big ]
                            -
                            \Big [
                                - x_je^{2x_j\beta_j}
                                + x_je^0
                                + x_je^0
                                - x_je^{-2x_j\beta_j}
                            \Big ]
                        }
                        \over
                        {
                            (
                                e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )^2
                        }
                    }
                $
            </p>

            <p>
                Apply rule such that $x^0 = 1$
                <br><br>
                $
                    =
                    {
                        {
                            \Big [
                                - x_je^{2x_j\beta_j}
                                -  x_j
                                -  x_j
                                -  x_je^{-2x_j\beta_j}
                            \Big ]
                            -
                            \Big [
                                - x_je^{2x_j\beta_j}
                                + x_j
                                + x_j
                                - x_je^{-2x_j\beta_j}
                            \Big ]
                        }
                        \over
                        {
                            (
                                e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )^2
                        }
                    }
                $
            </p>

            <p>
                Distribute negative sign to righthand expression in numerator
                <br><br>
                $
                    =
                    {
                        {
                            \Big [
                                - x_je^{2x_j\beta_j}
                                - x_j
                                - x_j
                                - x_je^{-2x_j\beta_j}
                            \Big ]
                            +
                            \Big [
                                + x_je^{2x_j\beta_j}
                                - x_j
                                - x_j
                                + x_je^{-2x_j\beta_j}
                            \Big ]
                        }
                        \over
                        {
                            (
                                e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )^2
                        }
                    }
                $
            </p>

            <p>
                Group like terms
                <br><br>
                $
                    =
                    {
                        {
                            (
                                x_je^{2x_j\beta_j}
                                - x_je^{2x_j\beta_j}
                            )
                            + 
                            (
                                - x_j
                                - x_j
                                - x_j
                                - x_j
                            )
                            + 
                            (
                                x_je^{-2x_j\beta_j}
                                - x_je^{-2x_j\beta_j}
                            )
                        }
                        \over
                        {
                            (
                                e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )^2
                        }
                    }
                $
            </p>

            <p>
                Cancel terms and Combine terms
                <br><br>
                $
                    =
                    {
                        {
                            \enclose{horizontalstrike}{
                            (
                                x_je^{2x_j\beta_j}
                                - x_je^{2x_j\beta_j}
                            )}
                            - 
                                4x_j
                            \space +
                            \enclose{horizontalstrike}{ 
                            (
                                x_je^{-2x_j\beta_j}
                                - x_je^{-2x_j\beta_j}
                            )}
                        }
                        \over
                        {
                            (
                                e^{x_j \beta_j} + e^{-x_j \beta_j}
                            )^2
                        }
                    }
                $
            </p>

            <p>
                Redefine result as $secth^2(x_j\beta_j)$. Note that: ${d \over dx}\Big(tanh(x)\Big) = {sech(x)^2}$<br>
                Since $sech(x) = {2 \over e^x + e^{-x}}$ then $sech^2(x) = {4 \over (e^x + e^{-x})^2}$ 
                <br><br>
                $
                    =
                    {
                        { -4x_j}
                        \over
                        { ( e^{x_j \beta_j} + e^{-x_j \beta_j} )^2 }
                    }
                $
                $
                    =
                    { 4 \over ( e^{x_j \beta_j} + e^{-x_j \beta_j} )^2 } \cdot -x_j
                $
                $
                    = {1 \over cos^2(x_j\beta_j) } \cdot -x_j
                $
                $
                    = sech^2(x_j\beta_j) \cdot -x_j
                $
                $
                    = -x_jsech^2(x_j\beta_j)
                $
            </p>













    </div>



        </div>
-->
        
        
        <script src='scripts/partA.js'></script>

        <script src="scripts/libs/bootstrap/jquery-3.2.1.slim.min.js"></script>
        <script src="scripts/libs/bootstrap/popper.min.js"></script>
        <script src="scripts/libs/bootstrap/bootstrap.min.js"></script>
    </body>
</html>